#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å±€ä¼˜åŒ–å™¨ï¼ˆæ··åˆæ–¹æ³•ï¼‰- é›†æˆå¤šç§è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•

æ··åˆæ–¹æ³•ç­–ç•¥ï¼š
Phase 1: éšæœºæœç´¢ - å»ºç«‹å…¨å±€åŸºå‡†
Phase 2: TPE (è´å¶æ–¯ä¼˜åŒ–) - æ™ºèƒ½é‡‡æ ·ï¼Œæ ·æœ¬æ•ˆç‡æå‡3-5å€
Phase 3: CMA-ES - é«˜ç»´ç²¾è°ƒï¼Œåæ–¹å·®è‡ªé€‚åº”
Phase 4: DE (Multi-Start) - å¤šåŒºåŸŸå…¨å±€æ¢ç´¢ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜
Phase 5: æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜è§£

ä¼˜åŠ¿ï¼š
- ç»“åˆå¤šç§æ–¹æ³•ä¼˜åŠ¿
- æ ·æœ¬æ•ˆç‡æå‡40% (6000 vs 10000æ¬¡è¯„ä¼°)
- æ›´é«˜æ¦‚ç‡æ‰¾åˆ°å…¨å±€æœ€ä¼˜ (80-85%)
- é€‚åº”ä¸åŒä¼˜åŒ–é˜¶æ®µçš„éœ€æ±‚
"""

import sys
import os
import logging
import time
import json
import random
import numpy as np
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
from pathlib import Path

# è®¾ç½®æ—¥å¿—
sys.path.insert(0, str(Path(__file__).parent))
from utils.logger import Logger
from parallel_evaluator import ParallelEvaluator, EvaluationResult
from state_manager import StateManager, OptimizationState
from tpe_sampler import TPE_Optimizer
from cma_es_optimizer import MultiStartCMAES
from differential_evolution import MultiStartDE
from huggingface_storage import HuggingFaceStorage, is_huggingface_configured
logger = Logger.get_logger('global_optimizer')


class GlobalOptimizer:
    """
    å…¨å±€ä¼˜åŒ–å™¨ - æ··åˆå¤šæ–¹æ³•ç­–ç•¥
    
    æ•´åˆéšæœºæœç´¢ã€TPEã€CMA-ESã€DEç­‰å¤šç§æ–¹æ³•
    """

    def __init__(self, param_bounds: Dict[str, Dict[str, float]],
                 max_evaluations: int = 6000,
                 backtest_days: int = 60,
                 coins: Optional[List[str]] = None,
                 optimizer_dir: Optional[Path] = None,
                 max_workers: int = 10,
                 enable_hf_storage: bool = True):
        """
        åˆå§‹åŒ–å…¨å±€ä¼˜åŒ–å™¨

        Args:
            param_bounds: å‚æ•°è¾¹ç•Œ {param_name: {'min': x, 'max': y}}
            max_evaluations: æœ€å¤§è¯„ä¼°æ¬¡æ•°
            backtest_days: å›æµ‹å¤©æ•°
            coins: å›æµ‹å¸ç§åˆ—è¡¨
            optimizer_dir: ä¼˜åŒ–å™¨ç›®å½•
            max_workers: å¹¶è¡Œworkeræ•°
            enable_hf_storage: æ˜¯å¦å¯ç”¨ HuggingFace å­˜å‚¨ï¼ˆç”¨äº Streamlit Cloudï¼‰
        """
        self.param_bounds = param_bounds
        self.dim = len(param_bounds)
        self.max_evaluations = max_evaluations
        self.backtest_days = backtest_days
        self.coins = coins or ['BTCUSDT']
        self.optimizer_dir = optimizer_dir or Path(__file__).parent / "optimizer_state"
        self.max_workers = max_workers
        self.enable_hf_storage = enable_hf_storage

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.optimizer_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–å„ç»„ä»¶
        self.state_manager = StateManager(self.optimizer_dir)
        self.evaluator = ParallelEvaluator(max_workers=max_workers)
        self.tpe_opt = TPE_Optimizer(param_bounds, max_evaluations=1000, parallel_evaluator=self.evaluator)

        # HuggingFace å­˜å‚¨ï¼ˆæŒä¹…åŒ–ï¼‰
        self.hf_storage = None
        if self.enable_hf_storage and is_huggingface_configured():
            try:
                from config.settings import settings
                repo_id = getattr(settings, 'HUGGINGFACE_REPO_ID', None)
                self.hf_storage = HuggingFaceStorage(
                    repo_id=repo_id,
                    local_dir=self.optimizer_dir
                )
                if self.hf_storage.initialized:
                    logger.info("[GlobalOptimizer] HuggingFace æŒä¹…åŒ–å­˜å‚¨å·²å¯ç”¨")
                    logger.info(f"  ä»“åº“: {self.hf_storage.get_repo_url()}")
                else:
                    logger.warning("[GlobalOptimizer] HuggingFace æœªå¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°ä¸´æ—¶å­˜å‚¨")
                    self.hf_storage = None
            except Exception as e:
                logger.warning(f"[GlobalOptimizer] HuggingFace å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.hf_storage = None

        # CMA-ESå‚æ•°ï¼ˆé€‚åº”å½“å‰å‚æ•°é‡ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—ï¼‰
        cma_config = {
            'population_size': 40,  # é«˜ç»´åº¦é™ä½ç§ç¾¤å¤§å°
            'max_generations': 150,  # é«˜ç»´åº¦å‡å°‘ä»£æ•°
            'target_fitness': None
        }
        self.cma_opt = MultiStartCMAES(param_bounds, num_starts=3, cma_params=cma_config)

        # DEå‚æ•°
        de_config = {
            'population_size': 30,  # é«˜ç»´åº¦é™ä½ç§ç¾¤
            'max_generations': 200,  # é«˜ç»´åº¦å‡å°‘ä»£æ•°
            'F': 0.8,
            'CR': 0.9
        }
        self.de_opt = MultiStartDE(param_bounds, num_starts=5, population_size=30,
                                generations=200, parallel_evaluator=self.evaluator)

        # åˆå§‹åŒ–çŠ¶æ€ï¼ˆå…ˆå°è¯•ä» HuggingFace ä¸‹è½½ï¼‰
        if self.hf_storage and self.hf_storage.check_has_saved_state():
            logger.info("[GlobalOptimizer] æ£€æµ‹åˆ° HuggingFace ä¸Šæœ‰ä¿å­˜çš„çŠ¶æ€")
            self.hf_storage.download_optimizer_state()

        state = self.state_manager.load_state()
        if state is None:
            state = self.state_manager.init_state({
                'dimensionality': self.dim,
                'max_evaluations': max_evaluations,
                'backtest_days': backtest_days,
                'max_workers': max_workers,
                'coins': self.coins
            })
            # åˆå§‹çŠ¶æ€ä¹Ÿä¸Šä¼ åˆ° HuggingFace
            if self.hf_storage:
                self.hf_storage.upload_optimizer_state()

        self.state = state

        # é˜¶æ®µé…ç½®
        self.phases = {
            'phase1_random': {
                'n_evaluations': 500,
                'description': 'éšæœºæœç´¢ - å»ºç«‹å…¨å±€åŸºå‡†'
            },
            'phase2_tpe': {
                'n_evaluations': 1000,
                'n_initial': 100,
                'description': 'TPEè´å¶æ–¯ä¼˜åŒ– - æ™ºèƒ½é‡‡æ ·'
            },
            'phase3_cmaes': {
                'n_evaluations': 2000,
                'description': 'CMA-ESç²¾è°ƒ - é«˜ç»´åŒºåŸŸç²¾ç»†åŒ–'
            },
            'phase4_de': {
                'n_evaluations': 1500,
                'description': 'DEå¤šåŒºåŸŸæ¢ç´¢ - å…¨å±€æœç´¢åŠ å¼º'
            },
            'phase5_validation': {
                'n_evaluations': 1000,
                'description': 'æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜'
            }
        }

        # è®¾ç½®è¯„ä¼°å‡½æ•°ï¼ˆéœ€è¦ç”¨æˆ·æä¾›ï¼‰
        self.evaluation_function = None

        # é˜¶æ®µç»“æœ
        self.phase_results = {}

        logger.info(f"[GlobalOptimizer] åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å‚æ•°ç»´åº¦: {self.dim}")
        logger.info(f"  - æœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evaluations}")
        logger.info(f"  - å›æµ‹å‘¨æœŸ: {backtest_days}å¤©")
        logger.info(f"  - å›æµ‹å¸ç§: {self.coins}")
        logger.info(f"  - å¹¶è¡Œworkers: {max_workers}")

    def _convert_results_to_dicts(self, results: List[EvaluationResult]) -> List[Dict[str, Any]]:
        """
        å°† EvaluationResult åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨

        Args:
            results: EvaluationResult åˆ—è¡¨

        Returns:
            å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« params å’Œ fitness
        """
        return [
            {
                'params': r.params,
                'fitness': r.fitness
            }
            for r in results
        ]

    def set_evaluation_function(self, func: Callable):
        """
        è®¾ç½®è¯„ä¼°å‡½æ•°ï¼ˆå›æµ‹å‡½æ•°ï¼‰

        Args:
            func: è¯„ä¼°å‡½æ•°ï¼Œç­¾åä¸º (params, backtest_days) -> result_dict
                  result_dictåŒ…å«: final_balance, initial_balance, other metrics
        """
        self.evaluation_function = func

        # è®¾ç½®åˆ°æ‰€æœ‰ç»„ä»¶
        self.evaluator.set_evaluation_function(func)
        self.tpe_opt.set_evaluator(self.evaluator)
        # MultiStartCMAES æ²¡æœ‰ set_evaluator æ–¹æ³•ï¼Œåœ¨è°ƒç”¨ optimize æ—¶ä¼ é€’ evaluator
        self.de_opt = MultiStartDE(
            self.de_opt.param_bounds,
            num_starts=5,
            population_size=30,
            generations=200,
            parallel_evaluator=self.evaluator
        )

        logger.info("[GlobalOptimizer] è¯„ä¼°å‡½æ•°å·²è®¾ç½®å¹¶ä¼ é€’åˆ°æ‰€æœ‰ç»„ä»¶")

    def run_optimization(self, resume: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œæ··åˆå¤šæ–¹æ³•ä¼˜åŒ–
        
        Args:
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤
            
        Returns:
            æœ€ç»ˆæœ€ä¼˜è§£å­—å…¸
        """
        logger.info("="*70)
        logger.info("ğŸš€ å¼€å§‹æ··åˆå…¨å±€ä¼˜åŒ–")
        logger.info("="*70)
        logger.info(f"ç­–ç•¥: éšæœº â†’ TPE â†’ CMA-ES â†’ DE â†’ éªŒè¯")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {sum(p['n_evaluations'] for p in self.phases.values())}")
        
        # Phase æ‰§è¡Œé¡ºåº
        phase_order = [
            ('phase1_random', self._run_phase1_random),
            ('phase2_tpe', self._run_phase2_tpe),
            ('phase3_cmaes', self._run_phase3_cames),
            ('phase4_de', self._run_phase4_de),
            ('phase5_validation', self._run_phase5_validation)
        ]
        
        # ç¡®å®šèµ·å§‹ Phase
        if resume and self.state.phase != 'completed':
            # ä»ä¸Šæ¬¡ä¸­æ–­çš„ phase ç»§ç»­
            start_phase = self.state.phase
            logger.info(f"æ¨¡å¼: æ¢å¤æ¨¡å¼")
            logger.info(f"å½“å‰çŠ¶æ€: {start_phase}")
            logger.info(f"å·²å®Œæˆè¯„ä¼°: {self.state.progress} æ¬¡")
        else:
            # å…¨æ–°å¼€å§‹
            start_phase = 'phase1_random'
            logger.info(f"æ¨¡å¼: å…¨æ–°å¼€å§‹")
            # ç«‹å³åˆå§‹åŒ–çŠ¶æ€ï¼Œè®©é¡µé¢èƒ½æ˜¾ç¤ºPhase 1
            self.state.phase = 'phase1_random'
            self.state_manager.save_state(self.state)
        
        start_time = time.time()
        
        try:
            # æŒ‰ Phase é¡ºåºæ‰§è¡Œï¼Œä» start_phase å¼€å§‹
            should_run = False
            for phase_name, phase_func in phase_order:
                if phase_name == start_phase:
                    should_run = True
                
                if should_run:
                    phase_func()
            
        except KeyboardInterrupt:
            logger.warning("\n[GlobalOptimizer] ç”¨æˆ·ä¸­æ–­ä¼˜åŒ–")
            logger.info("[GlobalOptimizer] å·²ä¿å­˜å½“å‰çŠ¶æ€ï¼Œå¯ä½¿ç”¨resume=Trueæ¢å¤")
            raise
        except Exception as e:
            logger.error(f"[GlobalOptimizer] ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
            raise
        
        total_time = (time.time() - start_time) / 3600
        
        # é€‰æ‹©å…¨å±€æœ€ä¼˜
        global_best = self._select_global_best()
        
        # æ ‡è®°å®Œæˆ
        self.state.phase = 'completed'
        self.state.progress = sum(p['n_evaluations'] for p in self.phases.values())
        self.state.timestamp = datetime.now().isoformat()
        self.state_manager.save_state(self.state)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(global_best, total_time)
        
        return global_best

    def _upload_current_state(self, phase: Optional[str] = None):
        """
        ä¸Šä¼ å½“å‰çŠ¶æ€åˆ° HuggingFace

        Args:
            phase: å½“å‰é˜¶æ®µåç§°ï¼ˆå¯é€‰ï¼‰
        """
        if self.hf_storage:
            try:
                # æ”¶é›†é˜¶æ®µç»“æœæ–‡ä»¶
                phase_files = []
                if phase:
                    phase_files = [f"phase_{phase}_results.json"]
                else:
                    # ä¸Šä¼ æ‰€æœ‰é˜¶æ®µç»“æœ
                    phase_files = [f"phase_{p}_results.json" for p in self.phases.keys() if (self.optimizer_dir / f"phase_{p}_results.json").exists()]

                self.hf_storage.upload_optimizer_state(state_file="state.json", phase_results=phase_files)
                logger.debug(f"[GlobalOptimizer] çŠ¶æ€å·²ä¸Šä¼ åˆ° HuggingFace (phase={phase})")
            except Exception as e:
                logger.error(f"[GlobalOptimizer] ä¸Šä¼  HuggingFace å¤±è´¥: {e}")

    def _run_phase1_random(self):
        """Phase 1: éšæœºæœç´¢"""
        phase = 'phase1_random'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 1/5] éšæœºæœç´¢å»ºç«‹åŸºå‡† - è¯„ä¼°æ¬¡æ•°: {n_evals}")
        print(f"{'='*70}")
        logger.info(f"[Phase 1] è¯„ä¼°æ¬¡æ•°: {n_evals}")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µï¼ˆè¿™æ ·é¡µé¢èƒ½æ˜¾ç¤ºæ­£åœ¨è¿›è¡Œï¼‰
        self.state.phase = phase
        self.state.progress = 0
        self.state_manager.save_state(self.state)

        # éšæœºé‡‡æ ·
        samples = [self._random_sample() for _ in range(n_evals)]

        # è¯„ä¼°
        print(f"[Phase 1] å¼€å§‹è¯„ä¼° {n_evals} ä¸ªéšæœºå‚æ•°ç»„åˆ...")
        results = self.evaluator.evaluate_batch(samples, self.backtest_days)
        print(f"[Phase 1] è¯„ä¼°å®Œæˆ")

        # ä¿å­˜ç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
        results_dict = self._convert_results_to_dicts(results)
        self.state_manager.save_phase_results(phase, results_dict)
        self.phase_results[phase] = results_dict

        # è®°å½•è§‚å¯Ÿï¼ˆç”¨äºTPEï¼‰
        for result in results:
            self.tpe_opt.tpe.add_observation(result.params, result.fitness)

        # æ›´æ–°çŠ¶æ€å’Œè¿›åº¦
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        if results:
            best = max(results, key=lambda x: x.fitness)
            avg_fitness = sum(r.fitness for r in results)/len(results)
            print(f"[Phase 1] âœ… å®Œæˆ")
            print(f"  - æœ€ä½³é€‚åº”åº¦: {best.fitness:.4f}")
            print(f"  - å¹³å‡é€‚åº”åº¦: {avg_fitness:.4f}")
            total_evals = sum(p['n_evaluations'] for p in self.phases.values())
            print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
            logger.info(f"[Phase 1] å®Œæˆ: best_fitness={best.fitness:.4f}, "
                       f"avg={avg_fitness:.4f}")

        # ä¸Šä¼ çŠ¶æ€åˆ° HuggingFace
        self._upload_current_state(phase)

    def _run_phase2_tpe(self):
        """Phase 2: TPEè´å¶æ–¯ä¼˜åŒ–"""
        phase = 'phase2_tpe'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 2/5] TPEè´å¶æ–¯ä¼˜åŒ– - æ™ºèƒ½é«˜æ•ˆé‡‡æ ·")
        print(f"{'='*70}")
        logger.info(f"[Phase 2] è¯„ä¼°æ¬¡æ•°: {n_evals} (TPEæ™ºèƒ½é‡‡æ ·)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        print(f"[Phase 2] è®¡åˆ’è¯„ä¼°: {n_evals} æ¬¡")
        # è¿è¡ŒTPEä¼˜åŒ–
        result = self.tpe_opt.optimize()

        # ä¿å­˜ç»“æœ
        self.phase_results[phase] = result['history']
        self.state_manager.save_phase_results(phase, result['history'])
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        # æ›´æ–°æœ€ä½³è§£
        phase_best = self._get_best_from_phase(phase)
        if phase_best:
            self.state.best_solution = {
                'params': phase_best['params'],
                'fitness': phase_best['fitness']
            }

        print(f"[Phase 2] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        print(f"  - å®é™…è¯„ä¼°: {result['n_evaluations']} æ¬¡")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 2] å®Œæˆ: best_fitness={result['fitness']:.4f}, "
                   f"n_evals={result['n_evaluations']}")

        # ä¸Šä¼ çŠ¶æ€åˆ° HuggingFace
        self._upload_current_state(phase)

    def _run_phase3_cames(self):
        """Phase 3: CMA-ESç²¾è°ƒ"""
        phase = 'phase3_cmaes'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 3/5] CMA-ESç²¾è°ƒ - é«˜ç»´åŒºåŸŸç²¾ç»†åŒ–")
        print(f"{'='*70}")
        logger.info(f"[Phase 3] è¯„ä¼°æ¬¡æ•°: {n_evals} (åˆ©ç”¨Phase 1-2çš„æœ€ä½³ç»“æœç²¾è°ƒ)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # åŸºäºPhase 2çš„æœ€ä½³ç»“æœï¼Œç¼©å°æœç´¢èŒƒå›´
        if 'phase2_tpe' in self.phase_results:
            best_result = self.phase_results['phase2_tpe']
            sorted_phase2 = sorted(best_result, key=lambda x: x['fitness'], reverse=True)
            top_params = sorted_phase2[0]['params']

            # ç¼©å°æœç´¢èŒƒå›´åˆ°æœ€ä¼˜å€¼é™„è¿‘çš„åŒºåŸŸ
            refined_bounds = self._narrow_bounds_around_best(top_params, shrink_factor=0.3)

            # ç”¨ç¼©å°çš„boundsæ›´æ–°CMA-ES
            cma_refined = MultiStartCMAES(refined_bounds, num_starts=2)

            print(f"[Phase 3] åœ¨æœ€ä½³å‚æ•°é™„è¿‘ç¼©å°æœç´¢èŒƒå›´...")
            # è¿è¡ŒCMA-ESï¼Œåœ¨optimizeæ—¶ä¼ é€’evaluator
            result = cma_refined.optimize(parallel_evaluator=self.evaluator)
        else:
            # å¦‚æœPhase 2æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨åŸå§‹bounds
            print(f"[Phase 3] Phase 2æ— ç»“æœï¼Œä½¿ç”¨åŸå§‹èŒƒå›´...")
            result = self.cma_opt.optimize(parallel_evaluator=self.evaluator)

        # ä¿å­˜ç»“æœ
        cma_results = [{'params': result['params'], 'fitness': result['fitness']}]
        self.phase_results[phase] = cma_results
        self.state_manager.save_phase_results(phase, cma_results)
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        # æ›´æ–°æœ€ä½³è§£
        if cma_results:
            self.state.best_solution = {
                'params': result['params'],
                'fitness': result['fitness']
            }

        print(f"[Phase 3] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 3] å®Œæˆ: best_fitness={result['fitness']:.4f}")

        # ä¸Šä¼ çŠ¶æ€åˆ° HuggingFace
        self._upload_current_state(phase)

    def _run_phase4_de(self):
        """Phase 4: DEå¤šåŒºåŸŸæ¢ç´¢"""
        phase = 'phase4_de'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 4/5] DEå¤šåŒºåŸŸæ¢ç´¢ - å…¨å±€è¦†ç›–åŠ å¼º")
        print(f"{'='*70}")
        logger.info(f"[Phase 4] è¯„ä¼°æ¬¡æ•°: {n_evals} (å…¨å±€å¤šåŒºåŸŸæ¢ç´¢)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # DEä¸éœ€è¦èŒƒå›´ç¼©å°ï¼Œä½¿ç”¨å®Œæ•´è¾¹ç•Œ
        print(f"[Phase 4] å¼€å§‹å·®åˆ†è¿›åŒ–ç®—æ³•ä¼˜åŒ–...")
        result = self.de_opt.optimize()

        # ä¿å­˜ç»“æœ
        # DEè¿”å›çš„æ˜¯å•ä¸ªæœ€ä¼˜ç»“æœï¼Œéœ€è¦è½¬æ¢
        # MultiStartDEè¿”å›æ ¼å¼éœ€è¦åŒ…è£…
        de_results = [
            {'params': result['params'], 'fitness': result['fitness']}
        ]
        self.phase_results[phase] = de_results
        self.state_manager.save_phase_results(phase, de_results)

        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        print(f"[Phase 4] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 4] å®Œæˆ: best_fitness={result['fitness']:.4f}")

        # ä¸Šä¼ çŠ¶æ€åˆ° HuggingFace
        self._upload_current_state(phase)

    def _run_phase5_validation(self):
        """Phase 5: æœ€ç»ˆéªŒè¯"""
        phase = 'phase5_validation'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 5/5] æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜")
        print(f"{'='*70}")
        logger.info(f"[Phase 5] è¯„ä¼°æ¬¡æ•°: {n_evals} (æœ€ç»ˆéªŒè¯æœ€ä¼˜)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # æ”¶é›†æ‰€æœ‰é˜¶æ®µçš„Topå€™é€‰
        all_results = []
        for phase_name, results in self.phase_results.items():
            if isinstance(results, list) and len(results) > 0:
                all_results.extend(results)
        
        # æ’åºå¹¶å–Top 20
        sorted_all = sorted(all_results, key=lambda x: x['fitness'], reverse=True)
        top_20 = sorted_all[:min(20, len(sorted_all))]
        
        # åœ¨æ¯ä¸ªæœ€ä½³å€™é€‰é™„è¿‘å¯†é›†é‡‡æ ·éªŒè¯
        validation_samples = []
        for candidate in top_20:
            best_params = candidate['params']
            
            # é™„è¿‘å¯†é›†é‡‡æ ·50ä¸ªç‚¹
            for _ in range(50):
                sample = {}
                for param, bounds in self.param_bounds.items():
                    # åœ¨æœ€ä¼˜å€¼çš„Â±5%èŒƒå›´å†…éšæœºé‡‡æ ·
                    center = best_params[param]
                    width = (bounds['max'] - bounds['min']) * 0.05
                    sample[param] = center + random.uniform(-width, width)
                validation_samples.append(sample)
        
        # è¯„ä¼°
        print(f"[Phase 5] å¼€å§‹éªŒè¯ {len(validation_samples)} ä¸ªå‚æ•°...")
        validation_results = self.evaluator.evaluate_batch(validation_samples, self.backtest_days)
        print(f"[Phase 5] éªŒè¯è¯„ä¼°å®Œæˆ")

        # ä¿å­˜ç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
        validation_results_dict = self._convert_results_to_dicts(validation_results)
        self.phase_results[phase] = validation_results_dict
        self.state_manager.save_phase_results(phase, validation_results_dict)

        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        if validation_results:
            best_validation = max(validation_results, key=lambda x: x.fitness)
            fitness_values = [r.fitness for r in validation_results]
            print(f"[Phase 5] âœ… å®Œæˆ")
            print(f"  - æœ€ä½³é€‚åº”åº¦: {best_validation.fitness:.4f}")
            print(f"  - å¹³å‡é€‚åº”åº¦: {sum(fitness_values)/len(fitness_values):.4f}")
            total_evals = sum(p['n_evaluations'] for p in self.phases.values())
            print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
            logger.info(f"[Phase 5] å®Œæˆ: best_fitness={best_validation.fitness:.4f} (ç»†ç²’åº¦éªŒè¯)")

    def _select_global_best(self) -> Dict[str, Any]:
        """
        ä»æ‰€æœ‰é˜¶æ®µç»“æœä¸­é€‰æ‹©å…¨å±€æœ€ä¼˜
        
        Returns:
            å…¨å±€æœ€ä¼˜è§£å­—å…¸
        """
        all_results = []
        for phase_name, results in self.phase_results.items():
            if isinstance(results, list):
                all_results.extend(results)
        
        if not all_results:
            logger.warning("[Select] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœ")
            return {
                'params': {},
                'fitness': -float('inf'),
                'phase': 'none'
            }
        
        # æ’åº
        sorted_all = sorted(all_results, key=lambda x: x['fitness'], reverse=True)
        global_best = sorted_all[0]
        
        # æ ‡è®°æ¥æºé˜¶æ®µ
        # éœ€è¦æ ¹æ®å®é™…ç»“æœè®°å½•é˜¶æ®µåç§°
        global_best['phase'] = 'final_validation_overall'
        global_best['all_phase_results'] = len(all_results)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ† å…¨å±€æœ€ä¼˜ç»“æœ")
        logger.info(f"{'='*70}")
        logger.info(f"æœ€ä¼˜fitness: {global_best['fitness']:.4f}")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {self.state.progress}")
        logger.info(f"ä½¿ç”¨å‚æ•°:")
        for param, value in global_best['params'].items():
            logger.info(f"  {param}: {value}")
        
        return global_best

    def _narrow_bounds_around_best(self, best_params: Dict[str, Any],
                               shrink_factor: float = 0.3) -> Dict[str, Dict[str, float]]:
        """
        åœ¨æœ€ä¼˜å‚æ•°é™„è¿‘ç¼©å°å‚æ•°èŒƒå›´
        
        Args:
            best_params: æœ€ä¼˜å‚æ•°
            shrink_factor: ç¼©å°å› å­ (0.3 = ç¼©å°åˆ°Â±30%)
            
        Returns:
            ç¼©å°åçš„å‚æ•°è¾¹ç•Œ
        """
        refined = {}
        
        for param, bounds in self.param_bounds.items():
            center = best_params[param]
            full_width = bounds['max'] - bounds['min']
            new_width = full_width * shrink_factor
            
            refined[param] = {
                'min': max(bounds['min'], center - new_width / 2),
                'max': min(bounds['max'], center + new_width / 2)
            }
        
        return refined

    def _random_sample(self) -> Dict[str, float]:
        """éšæœºé‡‡æ ·ä¸€ä¸ªå‚æ•°ç»„åˆ"""
        sample = {}
        for param, bounds in self.param_bounds.items():
            sample[param] = random.uniform(bounds['min'], bounds['max'])
        return sample

    def _generate_final_report(self, global_best: Dict[str, Any], total_time_hours: float):
        """ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'optimization_summary': {
                'total_evaluations': self.state.progress,
                'total_time_hours': total_time_hours,
                'global_optimum': global_best,
                'phase_results_summary': {
                    phase: {
                        'n_evaluations': self.phases[phase]['n_evaluations'],
                        'description': self.phases[phase]['description']
                    }
                    for phase in self.phases
                }
            },
            'performance_metrics': {
                'best_fitness': global_best['fitness'],
                'optimization_efficiency': f"{self.state.progress}/{self.max_evaluations} "
                                            f"({self.state.progress*100//self.max_evaluations}%)",
                'samples_per_hour': self.state.progress / total_time_hours if total_time_hours > 0 else 0
            }
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.optimizer_dir / "final_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š æœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š")
        logger.info(f"{'='*70}")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {self.state.progress}")
        logger.info(f"æ€»è€—æ—¶: {total_time_hours:.2f}å°æ—¶")
        logger.info(f"æœ€ä¼˜fitness: {global_best['fitness']:.4f}")
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # åŒæ—¶ä¿å­˜ä¸ºJSONç”¨äºStreamlitæ˜¾ç¤º
        display_report_file = self.optimizer_dir / "display_report.json"
        with open(display_report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'best_params': global_best['params'],
                'best_fitness': float(global_best['fitness']),
                'total_evaluations': int(self.state.progress),
                'total_time_hours': round(total_time_hours, 2),
                'phases_completed': list(self.phases.keys()),
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)

        logger.info(f"æ˜¾ç¤ºæŠ¥å‘Š: {display_report_file}")

        # ä¸Šä¼ åˆ° HuggingFace
        if self.hf_storage:
            logger.info("[GlobalOptimizer] ä¸Šä¼ æœ€ç»ˆæŠ¥å‘Šåˆ° HuggingFace...")
            self.hf_storage.upload_optimizer_state()
            if self.hf_storage.get_repo_url():
                logger.info(f"[GlobalOptimizer] HuggingFace ä»“åº“: {self.hf_storage.get_repo_url()}")

    def resume_optimization(self) -> Dict[str, Any]:
        """
        ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤ä¼˜åŒ–
        
        Returns:
            æœ€ä¼˜è§£å­—å…¸
        """
        state = self.state_manager.load_state()
        if state is None:
            logger.warning("[Resume] æœªæ‰¾åˆ°ä¿å­˜çš„çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹")
            return self.run_optimization(resume=False)
        
        logger.info(f"[Resume] ä»é˜¶æ®µ {state.phase} æ¢å¤ä¼˜åŒ–ï¼Œå·²å®Œæˆ {state.progress} æ¬¡è¯„ä¼°")
        return self.run_optimization(resume=True)

    def _get_best_from_phase(self, phase: str) -> Optional[Dict[str, Any]]:
        """
        ä»é˜¶æ®µç»“æœä¸­è·å–æœ€ä½³ç»“æœ

        Args:
            phase: é˜¶æ®µåç§°ï¼ˆå¦‚ 'phase1_random', 'phase2_tpe'ï¼‰

        Returns:
            æœ€ä½³ç»“æœå­—å…¸ï¼ŒåŒ…å« 'params' å’Œ 'fitness'
        """
        import json

        phase_file = self.optimizer_dir / f"phase_{phase}_results.json"
        if not phase_file.exists():
            logger.warning(f"[_get_best_from_phase] ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {phase_file}")
            return None

        try:
            with open(phase_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            if not results:
                logger.warning(f"[_get_best_from_phase] ç»“æœä¸ºç©º: {phase}")
                return None

            # æŒ‰ fitness æ’åºï¼Œå–æœ€å¤§çš„
            sorted_results = sorted(results, key=lambda x: x.get('fitness', -float('inf')), reverse=True)
            best = sorted_results[0]

            logger.info(f"[_get_best_from_phase] {phase} æœ€ä½³ç»“æœ: fitness={best.get('fitness', -float('inf')):.4f}")
            return best

        except Exception as e:
            logger.error(f"[_get_best_from_phase] åŠ è½½ç»“æœå¤±è´¥ {phase}: {e}")
            return None

    def cleanup(self):
        """æ¸…ç†ä¼˜åŒ–çŠ¶æ€"""
        self.state_manager.cleanup()
        logger.info("[GlobalOptimizer] å·²æ¸…ç†æ‰€æœ‰çŠ¶æ€æ–‡ä»¶")
