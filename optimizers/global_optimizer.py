#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å±€ä¼˜åŒ–å™¨ï¼ˆæ··åˆæ–¹æ³•ï¼‰- é›†æˆå¤šç§è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•

æ··åˆæ–¹æ³•ç­–ç•¥ï¼š
Phase 1: éšæœºæœç´¢ - å»ºç«‹å…¨å±€åŸºå‡†
Phase 2: TPE (è´å¶æ–¯ä¼˜åŒ–) - æ™ºèƒ½é‡‡æ ·ï¼Œæ ·æœ¬æ•ˆç‡æå‡3-5å€
Phase 3: CMA-ES - é«˜ç»´ç²¾è°ƒï¼Œåæ–¹å·®è‡ªé€‚åº”
Phase 4: DE (Multi-Start) - å¤šåŒºåŸŸå…¨å±€æ¢ç´¢ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜
Phase 5: æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜è§£

ä¼˜åŠ¿ï¼š
- ç»“åˆå¤šç§æ–¹æ³•ä¼˜åŠ¿
- æ ·æœ¬æ•ˆç‡æå‡40% (6000 vs 10000æ¬¡è¯„ä¼°)
- æ›´é«˜æ¦‚ç‡æ‰¾åˆ°å…¨å±€æœ€ä¼˜ (80-85%)
- é€‚åº”ä¸åŒä¼˜åŒ–é˜¶æ®µçš„éœ€æ±‚
"""

import sys
import os
import logging
import time
import json
import random
import numpy as np
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
from pathlib import Path

# è®¾ç½®æ—¥å¿—
sys.path.insert(0, str(Path(__file__).parent))
from utils.logger import Logger
from parallel_evaluator import ParallelEvaluator, EvaluationResult
from state_manager import StateManager, OptimizationState
from tpe_sampler import TPE_Optimizer
from cma_es_optimizer import MultiStartCMAES
from differential_evolution import MultiStartDE
logger = Logger.get_logger('global_optimizer')


class GlobalOptimizer:
    """
    å…¨å±€ä¼˜åŒ–å™¨ - æ··åˆå¤šæ–¹æ³•ç­–ç•¥
    
    æ•´åˆéšæœºæœç´¢ã€TPEã€CMA-ESã€DEç­‰å¤šç§æ–¹æ³•
    """

    def __init__(self, param_bounds: Dict[str, Dict[str, float]],
                 max_evaluations: int = 6000,
                 backtest_days: int = 60,
                 coins: Optional[List[str]] = None,
                 optimizer_dir: Optional[Path] = None,
                 max_workers: int = 10):
        """
        åˆå§‹åŒ–å…¨å±€ä¼˜åŒ–å™¨
        
        Args:
            param_bounds: å‚æ•°è¾¹ç•Œ {param_name: {'min': x, 'max': y}}
            max_evaluations: æœ€å¤§è¯„ä¼°æ¬¡æ•°
            backtest_days: å›æµ‹å¤©æ•°
            coins: å›æµ‹å¸ç§åˆ—è¡¨
            optimizer_dir: ä¼˜åŒ–å™¨ç›®å½•
            max_workers: å¹¶è¡Œworkeræ•°
        """
        self.param_bounds = param_bounds
        self.dim = len(param_bounds)
        self.max_evaluations = max_evaluations
        self.backtest_days = backtest_days
        self.coins = coins or ['BTCUSDT']
        self.optimizer_dir = optimizer_dir or Path(__file__).parent / "optimizer_state"
        self.max_workers = max_workers
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.optimizer_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.state_manager = StateManager(self.optimizer_dir)
        self.evaluator = ParallelEvaluator(max_workers=max_workers)
        self.tpe_opt = TPE_Optimizer(param_bounds, max_evaluations=1000, parallel_evaluator=self.evaluator)
        
        # CMA-ESå‚æ•°ï¼ˆé€‚åº”å½“å‰å‚æ•°é‡ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—ï¼‰
        cma_config = {
            'population_size': 40,  # é«˜ç»´åº¦é™ä½ç§ç¾¤å¤§å°
            'max_generations': 150,  # é«˜ç»´åº¦å‡å°‘ä»£æ•°
            'target_fitness': None
        }
        self.cma_opt = MultiStartCMAES(param_bounds, num_starts=3, cma_params=cma_config)
        
        # DEå‚æ•°
        de_config = {
            'population_size': 30,  # é«˜ç»´åº¦é™ä½ç§ç¾¤
            'max_generations': 200,  # é«˜ç»´åº¦å‡å°‘ä»£æ•°
            'F': 0.8,
            'CR': 0.9
        }
        self.de_opt = MultiStartDE(param_bounds, num_starts=5, population_size=30, 
                                generations=200, parallel_evaluator=self.evaluator)
        
        # åˆå§‹åŒ–çŠ¶æ€
        state = self.state_manager.load_state()
        if state is None:
            state = self.state_manager.init_state({
                'dimensionality': self.dim,
                'max_evaluations': max_evaluations,
                'backtest_days': backtest_days,
                'max_workers': max_workers,
                'coins': self.coins
            })
        
        self.state = state
        
        # é˜¶æ®µé…ç½®
        self.phases = {
            'phase1_random': {
                'n_evaluations': 500,
                'description': 'éšæœºæœç´¢ - å»ºç«‹å…¨å±€åŸºå‡†'
            },
            'phase2_tpe': {
                'n_evaluations': 1000,
                'n_initial': 100,
                'description': 'TPEè´å¶æ–¯ä¼˜åŒ– - æ™ºèƒ½é‡‡æ ·'
            },
            'phase3_cmaes': {
                'n_evaluations': 2000,
                'description': 'CMA-ESç²¾è°ƒ - é«˜ç»´åŒºåŸŸç²¾ç»†åŒ–'
            },
            'phase4_de': {
                'n_evaluations': 1500,
                'description': 'DEå¤šåŒºåŸŸæ¢ç´¢ - å…¨å±€æœç´¢åŠ å¼º'
            },
            'phase5_validation': {
                'n_evaluations': 1000,
                'description': 'æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤'
            }
        }

        # è®¾ç½®è¯„ä¼°å‡½æ•°ï¼ˆéœ€è¦ç”¨æˆ·æä¾›ï¼‰
        self.evaluation_function = None

        # é˜¶æ®µç»“æœ
        self.phase_results = {}

        logger.info(f"[GlobalOptimizer] åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å‚æ•°ç»´åº¦: {self.dim}")
        logger.info(f"  - æœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evaluations}")
        logger.info(f"  - å›æµ‹å‘¨æœŸ: {backtest_days}å¤©")
        logger.info(f"  - å›æµ‹å¸ç§: {self.coins}")
        logger.info(f"  - å¹¶è¡Œworkers: {max_workers}")

    def _convert_results_to_dicts(self, results: List[EvaluationResult]) -> List[Dict[str, Any]]:
        """
        å°† EvaluationResult åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨

        Args:
            results: EvaluationResult åˆ—è¡¨

        Returns:
            å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« params å’Œ fitness
        """
        return [
            {
                'params': r.params,
                'fitness': r.fitness
            }
            for r in results
        ]

    def set_evaluation_function(self, func: Callable):
        """
        è®¾ç½®è¯„ä¼°å‡½æ•°ï¼ˆå›æµ‹å‡½æ•°ï¼‰

        Args:
            func: è¯„ä¼°å‡½æ•°ï¼Œç­¾åä¸º (params, backtest_days) -> result_dict
                  result_dictåŒ…å«: final_balance, initial_balance, other metrics
        """
        self.evaluation_function = func

        # è®¾ç½®åˆ°æ‰€æœ‰ç»„ä»¶
        self.evaluator.set_evaluation_function(func)
        self.tpe_opt.set_evaluator(self.evaluator)
        # MultiStartCMAES æ²¡æœ‰ set_evaluator æ–¹æ³•ï¼Œåœ¨è°ƒç”¨ optimize æ—¶ä¼ é€’ evaluator
        self.de_opt = MultiStartDE(
            self.de_opt.param_bounds,
            num_starts=5,
            population_size=30,
            generations=200,
            parallel_evaluator=self.evaluator
        )

        logger.info("[GlobalOptimizer] è¯„ä¼°å‡½æ•°å·²è®¾ç½®å¹¶ä¼ é€’åˆ°æ‰€æœ‰ç»„ä»¶")

    def run_optimization(self, resume: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œæ··åˆå¤šæ–¹æ³•ä¼˜åŒ–
        
        Args:
            resume: æ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤
            
        Returns:
            æœ€ç»ˆæœ€ä¼˜è§£å­—å…¸
        """
        logger.info("="*70)
        logger.info("ğŸš€ å¼€å§‹æ··åˆå…¨å±€ä¼˜åŒ–")
        logger.info("="*70)
        logger.info(f"ç­–ç•¥: éšæœº â†’ TPE â†’ CMA-ES â†’ DE â†’ éªŒè¯")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {sum(p['n_evaluations'] for p in self.phases.values())}")
        
        if resume:
            logger.info(f"æ¨¡å¼: æ¢å¤æ¨¡å¼")
            logger.info(f"å½“å‰çŠ¶æ€: {self.state.phase}")
        else:
            logger.info(f"æ¨¡å¼: å…¨æ–°å¼€å§‹")
            # ç«‹å³åˆå§‹åŒ–çŠ¶æ€ï¼Œè®©é¡µé¢èƒ½æ˜¾ç¤ºPhase 1
            self.state.phase = 'phase1_random'
            self.state_manager.save_state(self.state)
        
        start_time = time.time()
        
        try:
            # ===== Phase 1: éšæœºæœç´¢ =====
            logger.info("\n" + "="*70)
            logger.info("Phase 1: éšæœºæœç´¢å»ºç«‹åŸºå‡†")
            logger.info("="*70)
            
            self._run_phase1_random()
            
            # ===== Phase 2: TPEè´å¶æ–¯ä¼˜åŒ– =====
            logger.info("\n" + "="*70)
            logger.info("Phase 2: TPEè´å¶æ–¯ä¼˜åŒ– - æ™ºèƒ½é«˜æ•ˆé‡‡æ ·")
            logger.info("="*70)
            
            self._run_phase2_tpe()
            
            # ===== Phase 3: CMA-ESç²¾è°ƒ =====
            logger.info("\n" "/"*70)
            logger.info("\\ Phase 3: CMA-ESç²¾è°ƒ - é«˜ç»´åŒºåŸŸç²¾ç»†åŒ–")
            logger.info("="*70)
            
            self._run_phase3_cames()
            
            # ===== Phase 4: DEå¤šåŒºåŸŸæ¢ç´¢ =====
            logger.info("\n" + "="*70)
            logger.info("Phase 4: DEå¤šåŒºåŸŸæ¢ç´¢ - å…¨å±€è¦†ç›–åŠ å¼º")
            logger.info("="*70)
            
            self._run_phase4_de()
            
            # ===== Phase 5: æœ€ç»ˆéªŒè¯ =====
            logger.info("\n" + "="*70)
            logger.info("Phase 5: æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜")
            logger.info("="*70)
            
            self._run_phase5_validation()
            
        except KeyboardInterrupt:
            logger.warning("\n[GlobalOptimizer] ç”¨æˆ·ä¸­æ–­ä¼˜åŒ–")
            logger.info("[GlobalOptimizer] å·²ä¿å­˜å½“å‰çŠ¶æ€ï¼Œå¯ä½¿ç”¨resume=Trueæ¢å¤")
            raise
        except Exception as e:
            logger.error(f"[GlobalOptimizer] ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
            raise
        
        total_time = (time.time() - start_time) / 3600
        
        # é€‰æ‹©å…¨å±€æœ€ä¼˜
        global_best = self._select_global_best()
        
        # æ ‡è®°å®Œæˆ
        self.state.phase = 'completed'
        self.state.progress = sum(p['n_evaluations'] for p in self.phases.values())
        self.state.timestamp = datetime.now().isoformat()
        self.state_manager.save_state(self.state)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(global_best, total_time)
        
        return global_best

    def _run_phase1_random(self):
        """Phase 1: éšæœºæœç´¢"""
        phase = 'phase1_random'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 1/5] éšæœºæœç´¢å»ºç«‹åŸºå‡† - è¯„ä¼°æ¬¡æ•°: {n_evals}")
        print(f"{'='*70}")
        logger.info(f"[Phase 1] è¯„ä¼°æ¬¡æ•°: {n_evals}")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µï¼ˆè¿™æ ·é¡µé¢èƒ½æ˜¾ç¤ºæ­£åœ¨è¿›è¡Œï¼‰
        self.state.phase = phase
        self.state.progress = 0
        self.state_manager.save_state(self.state)

        # éšæœºé‡‡æ ·
        samples = [self._random_sample() for _ in range(n_evals)]

        # è¯„ä¼°
        print(f"[Phase 1] å¼€å§‹è¯„ä¼° {n_evals} ä¸ªéšæœºå‚æ•°ç»„åˆ...")
        results = self.evaluator.evaluate_batch(samples, self.backtest_days)
        print(f"[Phase 1] è¯„ä¼°å®Œæˆ")

        # ä¿å­˜ç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
        results_dict = self._convert_results_to_dicts(results)
        self.state_manager.save_phase_results(phase, results_dict)
        self.phase_results[phase] = results_dict

        # è®°å½•è§‚å¯Ÿï¼ˆç”¨äºTPEï¼‰
        for result in results:
            self.tpe_opt.tpe.add_observation(result.params, result.fitness)

        # æ›´æ–°çŠ¶æ€å’Œè¿›åº¦
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        if results:
            best = max(results, key=lambda x: x.fitness)
            avg_fitness = sum(r.fitness for r in results)/len(results)
            print(f"[Phase 1] âœ… å®Œæˆ")
            print(f"  - æœ€ä½³é€‚åº”åº¦: {best.fitness:.4f}")
            print(f"  - å¹³å‡é€‚åº”åº¦: {avg_fitness:.4f}")
            total_evals = sum(p['n_evaluations'] for p in self.phases.values())
            print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
            logger.info(f"[Phase 1] å®Œæˆ: best_fitness={best.fitness:.4f}, "
                       f"avg={avg_fitness:.4f}")

    def _run_phase2_tpe(self):
        """Phase 2: TPEè´å¶æ–¯ä¼˜åŒ–"""
        phase = 'phase2_tpe'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 2/5] TPEè´å¶æ–¯ä¼˜åŒ– - æ™ºèƒ½é«˜æ•ˆé‡‡æ ·")
        print(f"{'='*70}")
        logger.info(f"[Phase 2] è¯„ä¼°æ¬¡æ•°: {n_evals} (TPEæ™ºèƒ½é‡‡æ ·)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        print(f"[Phase 2] è®¡åˆ’è¯„ä¼°: {n_evals} æ¬¡")
        # è¿è¡ŒTPEä¼˜åŒ–
        result = self.tpe_opt.optimize()

        # ä¿å­˜ç»“æœ
        self.phase_results[phase] = result['history']
        self.state_manager.save_phase_results(phase, result['history'])
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        # æ›´æ–°æœ€ä½³è§£
        phase_best = self._get_best_from_phase(phase)
        if phase_best:
            self.state.best_solution = {
                'params': phase_best['params'],
                'fitness': phase_best['fitness']
            }

        print(f"[Phase 2] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        print(f"  - å®é™…è¯„ä¼°: {result['n_evaluations']} æ¬¡")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 2] å®Œæˆ: best_fitness={result['fitness']:.4f}, "
                   f"n_evals={result['n_evaluations']}")

    def _run_phase3_cames(self):
        """Phase 3: CMA-ESç²¾è°ƒ"""
        phase = 'phase3_cmaes'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 3/5] CMA-ESç²¾è°ƒ - é«˜ç»´åŒºåŸŸç²¾ç»†åŒ–")
        print(f"{'='*70}")
        logger.info(f"[Phase 3] è¯„ä¼°æ¬¡æ•°: {n_evals} (åˆ©ç”¨Phase 1-2çš„æœ€ä½³ç»“æœç²¾è°ƒ)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # åŸºäºPhase 2çš„æœ€ä½³ç»“æœï¼Œç¼©å°æœç´¢èŒƒå›´
        if 'phase2_tpe' in self.phase_results:
            best_result = self.phase_results['phase2_tpe']
            sorted_phase2 = sorted(best_result, key=lambda x: x['fitness'], reverse=True)
            top_params = sorted_phase2[0]['params']

            # ç¼©å°æœç´¢èŒƒå›´åˆ°æœ€ä¼˜å€¼é™„è¿‘çš„åŒºåŸŸ
            refined_bounds = self._narrow_bounds_around_best(top_params, shrink_factor=0.3)

            # ç”¨ç¼©å°çš„boundsæ›´æ–°CMA-ES
            cma_refined = MultiStartCMAES(refined_bounds, num_starts=2)

            print(f"[Phase 3] åœ¨æœ€ä½³å‚æ•°é™„è¿‘ç¼©å°æœç´¢èŒƒå›´...")
            # è¿è¡ŒCMA-ESï¼Œåœ¨optimizeæ—¶ä¼ é€’evaluator
            result = cma_refined.optimize(parallel_evaluator=self.evaluator)
        else:
            # å¦‚æœPhase 2æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨åŸå§‹bounds
            print(f"[Phase 3] Phase 2æ— ç»“æœï¼Œä½¿ç”¨åŸå§‹èŒƒå›´...")
            result = self.cma_opt.optimize(parallel_evaluator=self.evaluator)

        # ä¿å­˜ç»“æœ
        cma_results = [{'params': result['params'], 'fitness': result['fitness']}]
        self.phase_results[phase] = cma_results
        self.state_manager.save_phase_results(phase, cma_results)
        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        # æ›´æ–°æœ€ä½³è§£
        if cma_results:
            self.state.best_solution = {
                'params': result['params'],
                'fitness': result['fitness']
            }

        print(f"[Phase 3] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 3] å®Œæˆ: best_fitness={result['fitness']:.4f}")

    def _run_phase4_de(self):
        """Phase 4: DEå¤šåŒºåŸŸæ¢ç´¢"""
        phase = 'phase4_de'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 4/5] DEå¤šåŒºåŸŸæ¢ç´¢ - å…¨å±€è¦†ç›–åŠ å¼º")
        print(f"{'='*70}")
        logger.info(f"[Phase 4] è¯„ä¼°æ¬¡æ•°: {n_evals} (å…¨å±€å¤šåŒºåŸŸæ¢ç´¢)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # DEä¸éœ€è¦èŒƒå›´ç¼©å°ï¼Œä½¿ç”¨å®Œæ•´è¾¹ç•Œ
        print(f"[Phase 4] å¼€å§‹å·®åˆ†è¿›åŒ–ç®—æ³•ä¼˜åŒ–...")
        result = self.de_opt.optimize()

        # ä¿å­˜ç»“æœ
        # DEè¿”å›çš„æ˜¯å•ä¸ªæœ€ä¼˜ç»“æœï¼Œéœ€è¦è½¬æ¢
        # MultiStartDEè¿”å›æ ¼å¼éœ€è¦åŒ…è£…
        de_results = [
            {'params': result['params'], 'fitness': result['fitness']}
        ]
        self.phase_results[phase] = de_results
        self.state_manager.save_phase_results(phase, de_results)

        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        print(f"[Phase 4] âœ… å®Œæˆ")
        print(f"  - æœ€ä½³é€‚åº”åº¦: {result['fitness']:.4f}")
        total_evals = sum(p['n_evaluations'] for p in self.phases.values())
        print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
        logger.info(f"[Phase 4] å®Œæˆ: best_fitness={result['fitness']:.4f}")

    def _run_phase5_validation(self):
        """Phase 5: æœ€ç»ˆéªŒè¯"""
        phase = 'phase5_validation'
        n_evals = self.phases[phase]['n_evaluations']

        print(f"\n{'='*70}")
        print(f"[Phase 5/5] æœ€ç»ˆéªŒè¯ - ç»†ç²’åº¦ç¡®è®¤æœ€ä¼˜")
        print(f"{'='*70}")
        logger.info(f"[Phase 5] è¯„ä¼°æ¬¡æ•°: {n_evals} (æœ€ç»ˆéªŒè¯æœ€ä¼˜)")

        # ç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå½“å‰é˜¶æ®µ
        self.state.phase = phase
        self.state_manager.save_state(self.state)

        # æ”¶é›†æ‰€æœ‰é˜¶æ®µçš„Topå€™é€‰
        all_results = []
        for phase_name, results in self.phase_results.items():
            if isinstance(results, list) and len(results) > 0:
                all_results.extend(results)
        
        # æ’åºå¹¶å–Top 20
        sorted_all = sorted(all_results, key=lambda x: x['fitness'], reverse=True)
        top_20 = sorted_all[:min(20, len(sorted_all))]
        
        # åœ¨æ¯ä¸ªæœ€ä½³å€™é€‰é™„è¿‘å¯†é›†é‡‡æ ·éªŒè¯
        validation_samples = []
        for candidate in top_20:
            best_params = candidate['params']
            
            # é™„è¿‘å¯†é›†é‡‡æ ·50ä¸ªç‚¹
            for _ in range(50):
                sample = {}
                for param, bounds in self.param_bounds.items():
                    # åœ¨æœ€ä¼˜å€¼çš„Â±5%èŒƒå›´å†…éšæœºé‡‡æ ·
                    center = best_params[param]
                    width = (bounds['max'] - bounds['min']) * 0.05
                    sample[param] = center + random.uniform(-width, width)
                validation_samples.append(sample)
        
        # è¯„ä¼°
        print(f"[Phase 5] å¼€å§‹éªŒè¯ {len(validation_samples)} ä¸ªå‚æ•°...")
        validation_results = self.evaluator.evaluate_batch(validation_samples, self.backtest_days)
        print(f"[Phase 5] éªŒè¯è¯„ä¼°å®Œæˆ")

        # ä¿å­˜ç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
        validation_results_dict = self._convert_results_to_dicts(validation_results)
        self.phase_results[phase] = validation_results_dict
        self.state_manager.save_phase_results(phase, validation_results_dict)

        self.state.phase = phase
        self.state.progress += n_evals
        self.state_manager.save_state(self.state)

        if validation_results:
            best_validation = max(validation_results, key=lambda x: x.fitness)
            fitness_values = [r.fitness for r in validation_results]
            print(f"[Phase 5] âœ… å®Œæˆ")
            print(f"  - æœ€ä½³é€‚åº”åº¦: {best_validation.fitness:.4f}")
            print(f"  - å¹³å‡é€‚åº”åº¦: {sum(fitness_values)/len(fitness_values):.4f}")
            total_evals = sum(p['n_evaluations'] for p in self.phases.values())
            print(f"  - ç´¯è®¡è¯„ä¼°: {self.state.progress}/{total_evals}")
            logger.info(f"[Phase 5] å®Œæˆ: best_fitness={best_validation.fitness:.4f} (ç»†ç²’åº¦éªŒè¯)")

    def _select_global_best(self) -> Dict[str, Any]:
        """
        ä»æ‰€æœ‰é˜¶æ®µç»“æœä¸­é€‰æ‹©å…¨å±€æœ€ä¼˜
        
        Returns:
            å…¨å±€æœ€ä¼˜è§£å­—å…¸
        """
        all_results = []
        for phase_name, results in self.phase_results.items():
            if isinstance(results, list):
                all_results.extend(results)
        
        if not all_results:
            logger.warning("[Select] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœ")
            return {
                'params': {},
                'fitness': -float('inf'),
                'phase': 'none'
            }
        
        # æ’åº
        sorted_all = sorted(all_results, key=lambda x: x['fitness'], reverse=True)
        global_best = sorted_all[0]
        
        # æ ‡è®°æ¥æºé˜¶æ®µ
        # éœ€è¦æ ¹æ®å®é™…ç»“æœè®°å½•é˜¶æ®µåç§°
        global_best['phase'] = 'final_validation_overall'
        global_best['all_phase_results'] = len(all_results)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ† å…¨å±€æœ€ä¼˜ç»“æœ")
        logger.info(f"{'='*70}")
        logger.info(f"æœ€ä¼˜fitness: {global_best['fitness']:.4f}")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {self.state.progress}")
        logger.info(f"ä½¿ç”¨å‚æ•°:")
        for param, value in global_best['params'].items():
            logger.info(f"  {param}: {value}")
        
        return global_best

    def _narrow_bounds_around_best(self, best_params: Dict[str, Any],
                               shrink_factor: float = 0.3) -> Dict[str, Dict[str, float]]:
        """
        åœ¨æœ€ä¼˜å‚æ•°é™„è¿‘ç¼©å°å‚æ•°èŒƒå›´
        
        Args:
            best_params: æœ€ä¼˜å‚æ•°
            shrink_factor: ç¼©å°å› å­ (0.3 = ç¼©å°åˆ°Â±30%)
            
        Returns:
            ç¼©å°åçš„å‚æ•°è¾¹ç•Œ
        """
        refined = {}
        
        for param, bounds in self.param_bounds.items():
            center = best_params[param]
            full_width = bounds['max'] - bounds['min']
            new_width = full_width * shrink_factor
            
            refined[param] = {
                'min': max(bounds['min'], center - new_width / 2),
                'max': min(bounds['max'], center + new_width / 2)
            }
        
        return refined

    def _random_sample(self) -> Dict[str, float]:
        """éšæœºé‡‡æ ·ä¸€ä¸ªå‚æ•°ç»„åˆ"""
        sample = {}
        for param, bounds in self.param_bounds.items():
            sample[param] = random.uniform(bounds['min'], bounds['max'])
        return sample

    def _generate_final_report(self, global_best: Dict[str, Any], total_time_hours: float):
        """ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'optimization_summary': {
                'total_evaluations': self.state.progress,
                'total_time_hours': total_time_hours,
                'global_optimum': global_best,
                'phase_results_summary': {
                    phase: {
                        'n_evaluations': self.phases[phase]['n_evaluations'],
                        'description': self.phases[phase]['description']
                    }
                    for phase in self.phases
                }
            },
            'performance_metrics': {
                'best_fitness': global_best['fitness'],
                'optimization_efficiency': f"{self.state.progress}/{self.max_evaluations} "
                                            f"({self.state.progress*100//self.max_evaluations}%)",
                'samples_per_hour': self.state.progress / total_time_hours if total_time_hours > 0 else 0
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.optimizer_dir / "final_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š æœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š")
        logger.info(f"{'='*70}")
        logger.info(f"æ€»è¯„ä¼°æ¬¡æ•°: {self.state.progress}")
        logger.info(f"æ€»è€—æ—¶: {total_time_hours:.2f}å°æ—¶")
        logger.info(f"æœ€ä¼˜fitness: {global_best['fitness']:.4f}")
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # åŒæ—¶ä¿å­˜ä¸ºJSONç”¨äºStreamlitæ˜¾ç¤º
        display_report_file = self.optimizer_dir / "display_report.json"
        with open(display_report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'best_params': global_best['params'],
                'best_fitness': float(global_best['fitness']),
                'total_evaluations': int(self.state.progress),
                'total_time_hours': round(total_time_hours, 2),
                'phases_completed': list(self.phases.keys())
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ˜¾ç¤ºæŠ¥å‘Š: {display_report_file}")

    def resume_optimization(self) -> Dict[str, Any]:
        """
        ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤ä¼˜åŒ–
        
        Returns:
            æœ€ä¼˜è§£å­—å…¸
        """
        state = self.state_manager.load_state()
        if state is None:
            logger.warning("[Resume] æœªæ‰¾åˆ°ä¿å­˜çš„çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹")
            return self.run_optimization(resume=False)
        
        logger.info(f"[Resume] ä»é˜¶æ®µ {state.phase} æ¢å¤ä¼˜åŒ–ï¼Œå·²å®Œæˆ {state.progress} æ¬¡è¯„ä¼°")
        return self.run_optimization(resume=True)

    def cleanup(self):
        """æ¸…ç†ä¼˜åŒ–çŠ¶æ€"""
        self.state_manager.cleanup()
        logger.info("[GlobalOptimizer] å·²æ¸…ç†æ‰€æœ‰çŠ¶æ€æ–‡ä»¶")
